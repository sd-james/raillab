---
title:  "Inter-and Intra-domain Knowledge Transfer for Related Tasks in Deep Character Recognition"
authors:
  - Nishai Kooverjee
  - Steven James
  - Terence Van Zyl

author_notes: []

publication: "*International SAUPEC/RobMech/PRASA Conference*"

date: 2020-01-29T00:00:00Z
publishDate: 2022-10-05T00:00:00Z

# conf = 1, journal = 2, preprint = 3, report = 4, book = 5, book chapter = 6, thesis = 7, patent = 9
# workshop = 9, symposium = 10, extended abstract =11
publication_types:
- "1"

abstract: "Pre-training a deep neural network on the ImageNet
  dataset is a common practice for training deep learning models,
  and generally yields improved performance and faster training
  times. The technique of pre-training on one task and then
  retraining on a new one is called transfer learning. In this paper
  we analyse the effectiveness of using deep transfer learning for
  character recognition tasks.We perform three sets of experiments
  with varying levels of similarity between source and target tasks
  to investigate the behaviour of different types of knowledge
  transfer. We transfer both parameters and features and analyse
  their behaviour. Our results demonstrate that no significant
  advantage is gained by using a transfer learning approach
  over a traditional machine learning approach for our character
  recognition tasks. This suggests that using transfer learning does
  not necessarily presuppose a better performing model in all cases."

featured: true
tags:
  - Deep Learning
  - Transfer Learning

image:
focal_point: ''
preview_only: false

slides: null

#url_dataset: "#"
#url_project: ""
#publication_short: ""
#url_source: "#"
#url_video: "#"
#url_slides: ""
#url_poster: "#"
#url_code: "#"
# doi: ""
---

