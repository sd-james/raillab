---
title:  "Should I Trust You? Incorporating Unreliable Expert Advice in Human-Agent Interaction"
authors:
  - Tamlin Love
  - Ritesh Ajoodha
  - Benjamin Rosman

author_notes: []

publication: "*Workshop on Human-aligned Reinforcement Learning for Autonomous Agents and Robots at ICDL*"

date: 2021-08-27T00:00:00Z
publishDate: 2022-10-05T00:00:00Z

# conf = 1, journal = 2, preprint = 3, report = 4, book = 5, book chapter = 6, thesis = 7, patent = 9
# workshop = 9, symposium = 10, extended abstract =11
publication_types:
  - "9"

abstract: A major concern in reinforcement learning, especially
  as it is applied to real-world and robotics problems, is that
  of sample-efficiency given increasingly complex problems and
  the difficulty of data acquisition in certain domains. To that end,
  many approaches incorporate external advice in the learning
  process in order to increase the rate at which an agent learns to
  solve a given problem. However, these approaches typically rely
  on a single reliable information source; the problem of learning
  with information from multiple, potentially unreliable sources
  is still an open question in assisted reinforcement learning. We
  present CLUE (Cautiously Learning with Unreliable Experts),
  a framework for learning single-stage decision problems with
  policy advice from multiple, potentially unreliable experts. We
  compare CLUE against an unassisted agent and an agent that
  na¨ıvely follows advice, and our results show that CLUE exhibits
  faster convergence than an unassisted agent when advised by
  reliable experts, but is nevertheless robust against incorrect
  advice from unreliable experts.



featured: true
tags:
  - Reinforcement Learning
  - Human-Computer Interaction


image:
  focal_point: ''
  preview_only: false

slides: null


---
